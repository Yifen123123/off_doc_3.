# -*- coding: utf-8 -*-
"""å…¬æ–‡RAG_9ç¨®.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V_1m7MwK8ti7nBhkPNj7QwaR6hjj5rUG
"""

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/

!cp -r /content/drive/MyDrive/rag_docs /content/

!pip install -U \
    langchain langchain-community langchain-core langchain-text-splitters \
    chromadb \
    sentence-transformers \
    transformers \
    huggingface_hub \
    aisuite \
    gradio

import os

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

import aisuite as ai
import gradio as gr

try:
    from google.colab import userdata
    api_key = userdata.get('Groq')
    if api_key:
        os.environ["GROQ_API_KEY"] = api_key
except Exception:
    pass

# å»ºç«‹ aisuite Clientï¼ˆGroqï¼‰
client = ai.Client()
MODEL_NAME = "groq:llama-3.3-70b-versatile"

"""##æƒæå‘é‡æ–‡ä»¶"""

BASE_DIR = "rag_docs"

subdirs = ["cases", "cocepts", "laws", "structure"]  # ç…§ä½ ç¾åœ¨çš„å‘½å

documents = []

for sub in subdirs:
    folder = os.path.join(BASE_DIR, sub)
    if not os.path.isdir(folder):
        print(f"âš ï¸ è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œç•¥éï¼š{folder}")
        continue

    for fname in os.listdir(folder):
        if not fname.endswith(".txt"):
            continue
        path = os.path.join(folder, fname)
        loader = TextLoader(path, encoding="utf-8")
        docs = loader.load()
        # è£œ metadataï¼šä¹‹å¾Œå¯ä»¥ç”¨ filter ç¯©æŸé¡è³‡æ–™
        for d in docs:
            d.metadata["category"] = sub
            d.metadata["filename"] = fname
        documents.extend(docs)

len(documents)

"""##åˆ‡ chunk + å»ºç«‹Chroma"""

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=200
)
split_docs = text_splitter.split_documents(documents)
print("Chunks æ•¸é‡ï¼š", len(split_docs))

embedding_model = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large"
)

CHROMA_DIR = "rag_chroma"

vectordb = Chroma.from_documents(
    documents=split_docs,
    embedding=embedding_model,
    persist_directory=CHROMA_DIR,
)

vectordb.persist()
print("âœ… Chroma å»ºå¥½ä¸¦å·² persist åœ¨", CHROMA_DIR)

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large"
)

vectordb = Chroma(
    embedding_function=embedding_model,
    persist_directory="rag_chroma"
)

DOC_TYPE_TO_STRUCTURE = {
    "æ‰£æŠ¼å‘½ä»¤": "æ‰£æŠ¼å‘½ä»¤_structure.txt",
    "ä¿å–®æŸ¥è©¢": "ä¿å–®æŸ¥è©¢_structure.txt",
    "ä¿å–®è¨»è¨˜": "ä¿å–®è¨»è¨˜_structure.txt",
    "ä¿å–®æŸ¥è©¢ï¼‹è¨»è¨˜": "ä¿å–®æŸ¥è©¢ï¼‹è¨»è¨˜_structure.txt",
    "æ”¶å–ä»¤": "æ”¶å–ä»¤_structure.txt",
    "æ’¤éŠ·ä»¤": "æ’¤éŠ·ä»¤_structure.txt",
    "æ”¶å–ï¼‹æ’¤éŠ·": "æ”¶å–ï¼‹æ’¤éŠ·_structure.txt",
    "é€šçŸ¥å‡½": "é€šçŸ¥å‡½_structure.txt",
    "å…¬è·æŸ¥è©¢": "å…¬è·æŸ¥è©¢_structure.txt",
}

STRUCTURE_DIR = os.path.join(BASE_DIR, "structure")

def load_structure_text(doc_type: str) -> str:
    fname = DOC_TYPE_TO_STRUCTURE.get(doc_type)
    if not fname:
        raise ValueError(f"æœªçŸ¥çš„ doc_type: {doc_type}")
    path = os.path.join(STRUCTURE_DIR, fname)
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def retrieve_context(query: str, k: int = 8, category: str | None = None) -> str:
    if category:
        docs = vectordb.similarity_search(
            query,
            k=k,
            filter={"category": category}
        )
    else:
        docs = vectordb.similarity_search(query, k=k)

    return "\n\n".join(
        f"[{d.metadata.get('category','')}/{d.metadata.get('filename','')}] {d.page_content}"
        for d in docs
    )

api_key = userdata.get("Groq")
os.environ["GROQ_API_KEY"] = api_key

client = ai.Client()
MODEL_NAME = "groq:openai/gpt-oss-20b"
#qwen/qwen3-32b
#llama-3.3-70b-versatile
#llama-3.1-8b-instant
#openai/gpt-oss-20b

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€å€‹å°ˆé–€è™•ç†å°ç£æ³•é™¢èˆ‡æ©Ÿé—œå…¬æ–‡çš„ã€Œæ¬„ä½æ“·å–åŠ©æ‰‹ã€ã€‚
ä½ çš„ä»»å‹™æ˜¯ï¼š
1. è®€æ‡‚å…¬æ–‡ã€‚
2. çµåˆæä¾›çš„æ³•æ¢ã€æ¦‚å¿µã€æ¡ˆä¾‹èªªæ˜ï¼ˆRAG contextï¼‰ã€‚
3. åš´æ ¼æŒ‰ç…§ã€Œè¼¸å‡ºæ ¼å¼è¦æ±‚ã€ç”¢å‡ºçµæœã€‚
4. ä¸è¦å¤šåŠ ä»»ä½•è§£é‡‹ã€å‰è¨€æˆ–å¾Œè¨˜ï¼Œåªè¼¸å‡ºæ“·å–çµæœæœ¬èº«ã€‚

è«‹ç”¨å°ç£ç¿’æ…£çš„ç¹é«”ä¸­æ–‡ã€‚
""".strip()


def extract_with_rag(raw_doc: str, doc_type: str, k: int = 8) -> str:
    # 1. è®€å–è©²é¡å‹çš„ structure èªªæ˜ï¼‹æ ¼å¼
    structure_text = load_structure_text(doc_type)

    # 2. è®“ RAG å¹«å¿™æ‰¾ç›¸é—œ laws / concepts / cases
    #    é€™è£¡ç”¨æ•´ä»½å…¬æ–‡ç•¶ queryï¼Œæ¯”è¼ƒç©©
    rag_context = retrieve_context(
        query=raw_doc,
        k=k,
        category=None,  # æƒ³åªç”¨ laws å¯æ”¹ "laws"ï¼Œæˆ–åªç”¨ "cases"
    )

    # 3. çµ„ user prompt
    user_prompt = f"""
å…¬æ–‡é¡å‹ï¼š{doc_type}

ã€å…¬æ–‡åŸæ–‡ã€‘ï¼š
{raw_doc}

ã€ç›¸é—œçŸ¥è­˜ï¼ˆæ³•å¾‹ã€æ¦‚å¿µã€æ¡ˆä¾‹ç­‰ï¼Œåƒ…ä¾›ä½ åƒè€ƒæ¨ç†ï¼‰ã€‘ï¼š
{rag_context}

ã€è¼¸å‡ºæ ¼å¼èˆ‡æ¬„ä½èªªæ˜ã€‘ï¼š
{structure_text}

è«‹ä½ ä¾ç…§ã€Œè¼¸å‡ºæ ¼å¼è¦æ±‚ã€ï¼Œå°é€™ä»½å…¬æ–‡åšæ¬„ä½æ“·å–èˆ‡æ•´ç†ï¼š
- åƒ…è¼¸å‡ºæ•´ç†å¥½çš„çµæœæœ¬èº«ã€‚
- è‹¥æŸæ¬„ä½åŸæ–‡ä¸­æ²’æœ‰æ˜ç¢ºè³‡è¨Šï¼Œè«‹å¡«ã€Œç„¡æ˜ç¢ºè¨˜è¼‰ã€æˆ–ä¾çµæ§‹æª”çš„æŒ‡ç¤ºè™•ç†ã€‚
    """.strip()

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
    )
    return response.choices[0].message.content

pip install groq

import gradio as gr

DOC_TYPES = list(DOC_TYPE_TO_STRUCTURE.keys())  # Dropdown ç”¨

def ui_extract(doc_text, doc_type):
    if not doc_text.strip():
        return "è«‹å…ˆè²¼ä¸Šå®Œæ•´å…¬æ–‡ã€‚"
    return extract_with_rag(doc_text, doc_type)

with gr.Blocks() as demo:
    gr.Markdown("# ğŸ“„ ä¹é¡å…¬æ–‡æ“·å– RAG æ¸¬è©¦ç‰ˆ")

    with gr.Row():
        doc_type = gr.Dropdown(
            choices=DOC_TYPES,
            value="æ‰£æŠ¼å‘½ä»¤",
            label="å…¬æ–‡é¡å‹"
        )
    doc_input = gr.Textbox(
        lines=18,
        label="è«‹è²¼ä¸Šå®Œæ•´å…¬æ–‡åŸæ–‡",
        placeholder="ä¾‹å¦‚ï¼š\nè‡ºç£â—‹â—‹åœ°æ–¹æ³•é™¢æ°‘äº‹åŸ·è¡Œè™• åŸ·è¡Œå‘½ä»¤..."
    )
    output = gr.Textbox(
        lines=20,
        label="æ“·å–çµæœ"
    )

    btn = gr.Button("é–‹å§‹æ“·å–")
    btn.click(ui_extract, inputs=[doc_input, doc_type], outputs=output)

demo.launch(debug=True)

