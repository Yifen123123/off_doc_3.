import os, json, re
import gradio as gr
from typing import Optional, Dict, Any

from fastapi import FastAPI
from pydantic import BaseModel
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_ollama import ChatOllama


# åŸºæœ¬è¨­å®š

BASE_DIR = "rag_docs"
subdirs = ["cases", "cocepts", "laws", "structure"]  # è·Ÿä½ åŸæœ¬ä¸€è‡´
STRUCTURE_DIR = os.path.join(BASE_DIR, "structure")
CHROMA_DIR = "rag_chroma"

OLLAMA_MODEL = "qwen2.5:14b-instruct"  
llm = ChatOllama(model=OLLAMA_MODEL,temperature=0,)                                                 

embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")


# å»ºç«‹ Chroma å‘é‡åº«

documents = []
for sub in subdirs:
    folder = os.path.join(BASE_DIR, sub)
    for fname in os.listdir(folder):
        if fname.endswith(".txt"):
            path = os.path.join(folder, fname)
            docs = TextLoader(path, encoding="utf-8").load()
            for d in docs:
                d.metadata["category"] = sub
                d.metadata["filename"] = fname
            documents.extend(docs)

split_docs = RecursiveCharacterTextSplitter(
    chunk_size=800, chunk_overlap=200
).split_documents(documents)

vectordb = Chroma.from_documents(
    documents=split_docs,
    embedding=embedding_model,
    persist_directory=CHROMA_DIR,
)

def retrieve_context(query: str, k: int = 6, category=None) -> str:
    if category:
        docs = vectordb.similarity_search(query, k=k, filter={"category": category})
    else:
        docs = vectordb.similarity_search(query, k=k)

    return "\n\n".join(
        f"[{d.metadata.get('category','')}/{d.metadata.get('filename','')}] {d.page_content}"
        for d in docs
    )



# ç¬¬äºŒéƒ¨åˆ†ï¼šstructure å°æ‡‰

DOC_TYPE_TO_STRUCTURE = {
    "æ‰£æŠ¼å‘½ä»¤": "æ‰£æŠ¼å‘½ä»¤_structure.txt",
    "ä¿å–®æŸ¥è©¢": "ä¿å–®æŸ¥è©¢_structure.txt",
    "ä¿å–®è¨»è¨˜": "ä¿å–®è¨»è¨˜_structure.txt",
    "ä¿å–®æŸ¥è©¢ï¼‹è¨»è¨˜": "ä¿å–®æŸ¥è©¢ï¼‹è¨»è¨˜_structure.txt",
    "æ”¶å–ä»¤": "æ”¶å–ä»¤_structure.txt",
    "æ’¤éŠ·ä»¤": "æ’¤éŠ·ä»¤_structure.txt",
    "æ”¶å–ï¼‹æ’¤éŠ·": "æ”¶å–ï¼‹æ’¤éŠ·_structure.txt",
    "é€šçŸ¥å‡½": "é€šçŸ¥å‡½_structure.txt",
    "å…¬è·æŸ¥è©¢": "å…¬è·æŸ¥è©¢_structure.txt",
}


def load_detail_structure_text(doc_type: str) -> str:
    fname = DOC_TYPE_TO_DETAIL_STRUCTURE[doc_type]
    path = os.path.join(STRUCTURE_DIR, fname)
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def safe_parse_json(text: str):
    t = text.strip().replace("```json", "").replace("```", "").strip()
    try:
        return json.loads(t)
    except:
        return {"raw_output": t}



# Part 1ï¼šåŸºæœ¬å››æ¬„ä½ï¼ˆLLM + è¦å‰‡ï¼‰

SYSTEM_PROMPT_BASIC = """
ä½ æ˜¯ä¸€å€‹å°ç£æ³•é™¢/æ©Ÿé—œå…¬æ–‡çš„ã€ŒåŸºæœ¬æ¬„ä½æ“·å–åŠ©æ‰‹ã€ã€‚
åªæ“·å–ï¼š
- åŸºæº–æ—¥
- ä¾†å‡½æ©Ÿé—œ
- æ”¶æ–‡ç·¨è™Ÿï¼ˆç´”æ•¸å­—ã€åœ¨å‰åŠéƒ¨ã€ä¸èƒ½æ˜¯é›»è©±/å‚³çœŸ/ä¿å–®ï¼‰
- æŸ¥è©¢å°è±¡ï¼ˆå§“åï¼‹èº«åˆ†è­‰å­—è™Ÿï¼Œè¼¸å‡ºæ ¼å¼ï¼šå§“å, èº«åˆ†è­‰ï¼‰
è‹¥æ‰¾ä¸åˆ°ã€Œæ—¥æœŸè¡Œ + é„°è¿‘ç´”æ•¸å­—æ”¶æ–‡è™Ÿã€é…å°ï¼ŒåŸºæº–æ—¥èˆ‡æ”¶æ–‡ç·¨è™Ÿéƒ½å¡«ã€Œç„¡æ˜ç¢ºè¨˜è¼‰ã€ã€‚
åš´æ ¼è¼¸å‡º JSONï¼Œä¸è¦å¤šé¤˜æ–‡å­—ã€‚
""".strip()

DATE_PATTERNS = [
    r"(?:ä¸­è¯æ°‘åœ‹)?\s*\d{2,4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥",
    r"\b\d{2,4}[./-]\d{1,2}[./-]\d{1,2}\b",
]

BANNED_KEYWORDS = [
    "é›»è©±","TEL","Tel","tel","å‚³çœŸ","FAX","Fax","fax",
    "ä¿å–®","ä¿éšª","ä¿å–®è™Ÿ","ä¿å–®ç·¨è™Ÿ","å¸³è™Ÿ","å¸³æˆ¶","åºè™Ÿ",
    "è¯çµ¡","æ‰‹æ©Ÿ","å¸‚è©±","çµ±ä¸€ç·¨è™Ÿ","çµ±ç·¨"
]

def _is_date_line(line: str) -> bool:
    return any(re.search(p, line.strip()) for p in DATE_PATTERNS)

def _is_pure_number_line(line: str) -> bool:
    s = line.strip().replace(" ", "")
    return s.isdigit() and len(s) >= 5

def _has_banned_keyword(line: str) -> bool:
    return any(k in line for k in BANNED_KEYWORDS)

def find_date_and_receipt_pair(raw_doc: str):
    lines = [ln.strip() for ln in raw_doc.splitlines() if ln.strip()]
    if not lines:
        return None, None
    head_lines = lines[: max(1, len(lines)//2)]

    for i, line in enumerate(head_lines):
        if not _is_date_line(line):
            continue
        for j in (i+1, i+2):
            if j >= len(head_lines):
                continue
            cand = head_lines[j]
            if _has_banned_keyword(cand):
                continue
            if _is_pure_number_line(cand):
                return line, cand
    return None, None

def normalize_target_field(target: str) -> str:
    id_match = re.search(r"[A-Z][0-9]{9}", target, re.I)
    id_number = id_match.group(0).upper() if id_match else None
    name_match = re.search(r"[\u4e00-\u9fa5]{2,4}", target)
    name = name_match.group(0) if name_match else None
    if name and id_number:
        return f"{name}, {id_number}"
    return target

def extract_basic_info_with_llm(raw_doc: str) -> dict:
    prompt = f"""
{SYSTEM_PROMPT_BASIC}

ã€å…¬æ–‡åŸæ–‡ã€‘
{raw_doc}

è«‹å›å‚³ JSONï¼š
{{
  "åŸºæº–æ—¥": "",
  "ä¾†å‡½æ©Ÿé—œ": "",
  "æ”¶æ–‡ç·¨è™Ÿ": "",
  "æŸ¥è©¢å°è±¡": ""
}}
""".strip()

    resp = llm.invoke(prompt).content.strip()
    cleaned = resp.replace("```json","").replace("```","").strip()
    data = json.loads(cleaned)

    dline, rno = find_date_and_receipt_pair(raw_doc)
    if dline and rno:
        data["åŸºæº–æ—¥"] = dline
        data["æ”¶æ–‡ç·¨è™Ÿ"] = rno
    else:
        data["åŸºæº–æ—¥"] = "ç„¡æ˜ç¢ºè¨˜è¼‰"
        data["æ”¶æ–‡ç·¨è™Ÿ"] = "ç„¡æ˜ç¢ºè¨˜è¼‰"

    if isinstance(data.get("æŸ¥è©¢å°è±¡"), str):
        data["æŸ¥è©¢å°è±¡"] = normalize_target_field(data["æŸ¥è©¢å°è±¡"])

    return data



# Part 2ï¼šèªªæ˜æ¬„ä½ï¼ˆRAG + detail structureï¼‰

SYSTEM_PROMPT_DETAIL = """
ä½ æ˜¯ä¸€å€‹å°ˆé–€è™•ç†å°ç£æ³•é™¢èˆ‡æ©Ÿé—œå…¬æ–‡çš„ã€Œæ¬„ä½æ“·å–åŠ©æ‰‹ã€ã€‚
ä½ çš„ä»»å‹™ï¼š
1. è®€æ‡‚å…¬æ–‡ã€‚
2. çµåˆæä¾›çš„æ³•æ¢ã€æ¦‚å¿µã€æ¡ˆä¾‹èªªæ˜ï¼ˆRAG contextï¼‰ã€‚
3. åš´æ ¼æŒ‰ç…§ã€Œè¼¸å‡ºæ ¼å¼è¦æ±‚ã€ç”¢å‡ºçµæœã€‚
4. ä¸è¦å¤šåŠ ä»»ä½•è§£é‡‹ã€å‰è¨€æˆ–å¾Œè¨˜ï¼Œåªè¼¸å‡ºæ“·å–çµæœæœ¬èº«ã€‚
""".strip()


def extract_with_rag(raw_doc: str, doc_type: str, k: int = 8) -> str:
    structure_text = load_structure_text(doc_type)
    rag_context = retrieve_context(query=raw_doc, k=k, category=None)

    user_prompt = f"""
å…¬æ–‡é¡å‹ï¼š{doc_type}

ã€å…¬æ–‡åŸæ–‡ã€‘
{raw_doc}

ã€ç›¸é—œçŸ¥è­˜ï¼ˆæ³•å¾‹ã€æ¦‚å¿µã€æ¡ˆä¾‹ç­‰ï¼Œåƒ…ä¾›ä½ åƒè€ƒæ¨ç†ï¼‰ã€‘
{rag_context}

ã€è¼¸å‡ºæ ¼å¼èˆ‡æ¬„ä½èªªæ˜ï¼ˆJSON schemaï¼‰ã€‘
{structure_text}

è«‹ä¾ç…§ schema è¼¸å‡ºç´” JSONï¼Œåªèƒ½åŒ…å« schema è£¡çš„ keysã€‚
è‹¥ç„¡æ˜ç¢ºè¨˜è¼‰å¡«ã€Œç„¡æ˜ç¢ºè¨˜è¼‰ã€ã€‚
""".strip()

    return llm.invoke(user_prompt).content.strip()


# ä½ è¦åˆªæ‰çš„ã€ŒåŸºæœ¬æ¬„ä½ã€keys
BASIC_LIKE_KEYS = {
    "åŸºæº–æ—¥", "ç™¼æ–‡æ—¥æœŸ",
    "ä¾†å‡½æ©Ÿé—œ", "ç™¼æ–‡æ©Ÿé—œ",
    "æ”¶æ–‡ç·¨è™Ÿ",
    "æŸ¥è©¢å°è±¡",
    "ä¾†å‡½è³‡è¨Š", "æ‰¿è¾¦è³‡è¨Š"
}


def prune_basic_from_detail(obj):
    """éè¿´åˆªæ‰ details è£¡çš„åŸºæœ¬æ¬„ä½"""
    if isinstance(obj, dict):
        new_obj = {}
        for k, v in obj.items():
            if k in BASIC_LIKE_KEYS:
                continue
            new_obj[k] = prune_basic_from_detail(v)
        return new_obj
    elif isinstance(obj, list):
        return [prune_basic_from_detail(x) for x in obj]
    else:
        return obj


def extract_all_fields(raw_doc: str, doc_type: str):
    basic = extract_basic_info_with_llm(raw_doc)

    detail_text = extract_with_rag(raw_doc, doc_type)
    detail_json = safe_parse_json(detail_text)
    detail_clean = prune_basic_from_detail(detail_json)

    return basic, detail_clean


# # Gradio UI

# DOC_TYPES = list(DOC_TYPE_TO_STRUCTURE.keys())

# def ui_extract(doc_text, doc_type):
#     if not doc_text.strip():
#         return {"error": "è«‹å…ˆè²¼ä¸Šå®Œæ•´å…¬æ–‡"}, {"error": "è«‹å…ˆè²¼ä¸Šå®Œæ•´å…¬æ–‡"}

#     return extract_all_fields(doc_text, doc_type)


# with gr.Blocks() as demo:
#     gr.Markdown("# ğŸ“„ ä¹é¡å…¬æ–‡æ“·å–ï¼ˆæœ¬åœ° Ollama + åŸ structure + prune åŸºæœ¬æ¬„ä½ï¼‰")

#     doc_type = gr.Dropdown(choices=DOC_TYPES, value="æ‰£æŠ¼å‘½ä»¤", label="å…¬æ–‡é¡å‹")
#     doc_input = gr.Textbox(lines=18, label="è«‹è²¼ä¸Šå®Œæ•´å…¬æ–‡åŸæ–‡ï¼ˆOCRæ–‡å­—ï¼‰")
#     btn = gr.Button("é–‹å§‹æ“·å–")

#     with gr.Tabs():
#         with gr.Tab("åŸºæœ¬æ¬„ä½ï¼ˆå››é …ï¼‰"):
#             basic_out = gr.JSON()
#         with gr.Tab("èªªæ˜æ¬„ä½ï¼ˆå·²å»é™¤åŸºæœ¬æ¬„ä½ï¼‰"):
#             detail_out = gr.JSON()

#     btn.click(ui_extract, inputs=[doc_input, doc_type], outputs=[basic_out, detail_out])

# demo.launch()

#FastAPIï¼šå…©å€‹ endpoint

app = FastAPI(title="RAG å…¬æ–‡æ“·å– API")

class BasicRequest(BaseModel):
    text: str

class DetailRequest(BaseModel):
    text: str
    doc_type: str
    k: int = 8

@app.post("/extract_basic")
def extract_basic(req: BasicRequest):
    return extract_basic_info_with_llm(req.text)

@app.post("/extract_detail")
def extract_detail(req: DetailRequest):
    return extract_detail_only(req.text, req.doc_type, k=req.k)

# ï¼ˆå¯é¸ï¼‰å¦‚æœä½ ä¹Ÿæƒ³è¦ä¸€å€‹ä¸€æ¬¡å›å…©æ®µçš„
@app.post("/extract_all")
def extract_all(req: DetailRequest):
    basic = extract_basic_info_with_llm(req.text)
    detail = extract_detail_only(req.text, req.doc_type, k=req.k)
    return {"åŸºæœ¬æ¬„ä½": basic, "èªªæ˜æ¬„ä½": detail}

#æ‰“é–‹ http://localhost:8000/docs