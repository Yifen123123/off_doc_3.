import os
import sys
import shutil
from typing import List, Tuple

# --- è®“ requests/hf_hub ä½¿ç”¨ç³»çµ± CA æ†‘è­‰ï¼ˆé¿å… SSL é©—è­‰å•é¡Œï¼‰---
import certifi
os.environ.setdefault("REQUESTS_CA_BUNDLE", certifi.where())
os.environ.setdefault("SSL_CERT_FILE", certifi.where())
# å•Ÿç”¨æ›´å¿«çš„å‚³è¼¸ï¼ˆè‹¥å·²å®‰è£ hf_transferï¼‰
os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")
# è‹¥ä½ çš„å…¬å¸å·²åœ¨ç³»çµ±å±¤è¨­å®š HTTP(S)_PROXYï¼Œhuggingface_hub æœƒè‡ªå‹•ä¿¡ä»»å®ƒ

from huggingface_hub import snapshot_download, hf_hub_download
from sentence_transformers import SentenceTransformer

# LangChain
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
try:
    # å»ºè­°ï¼šæ–°å¥—ä»¶ï¼ˆè‹¥æœªå®‰è£æœƒé€²åˆ° exceptï¼‰
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except Exception:
    # ç›¸å®¹èˆŠç‰ˆ
    from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.vectorstores import FAISS


# =====================
# è¨­å®š
# =====================
UPLOAD_DIR = "uploaded_docs"
DB_DIR = "faiss_db"
DB_ZIP = "faiss_db.zip"

# ä½ è¦çš„æ¨¡å‹ï¼ˆå¯æ”¹æˆ base æˆ– MiniLM ç•¶å‚™æ´ï¼‰
PRIMARY_MODEL_ID = "intfloat/multilingual-e5-large"
BACKUP_MODEL_IDS = [
    "intfloat/multilingual-e5-base",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
]

# å»ºè³‡æ–™å¤¾
os.makedirs(UPLOAD_DIR, exist_ok=True)
print(f"è«‹å°‡ .txt / .pdf / .docx æ”¾åˆ°ï¼š{UPLOAD_DIR}")


# =====================
# å·¥å…·ï¼šä¸‹è¼‰ + æ±ºå®šæ¨¡å‹è·¯å¾‘
# =====================
def try_snapshot_download(model_id: str, local_dir: str) -> str:
    """
    ç”¨ snapshot_download æŠŠæ•´å€‹æ¨¡å‹æŠ“åˆ° local_dirã€‚
    æˆåŠŸå›å‚³å¯¦éš›å­˜æ”¾è·¯å¾‘ï¼›å¤±æ•—æœƒä¸Ÿä¾‹å¤–ã€‚
    """
    path = snapshot_download(
        repo_id=model_id,
        local_dir=local_dir,
        local_dir_use_symlinks=False,
        resume_download=True,
    )
    return path


def gentle_probe_with_hf_hub_download(model_id: str) -> None:
    """
    é€é hf_hub_download ä¸‹è¼‰ä¸€å€‹å°æª”æ¡ˆï¼ˆä¾‹å¦‚ config.jsonï¼‰ä¾†æ¸¬è©¦ proxy/SSLï¼Œ
    æŒ‡å®š trust_env=True è¡¨ç¤ºä¿¡ä»»ç³»çµ± proxy è¨­å®šã€‚
    æˆåŠŸå‰‡ä»£è¡¨é€£ç·š OKï¼›å¤±æ•—æœƒä¸Ÿä¾‹å¤–ã€‚
    """
    # ä¸åŒæ¨¡å‹æª”åç•¥æœ‰å·®ç•°ï¼Œå„ªå…ˆå˜—è©¦å¸¸è¦‹ config.json
    candidates = ["config.json", "model.safetensors", "modules.json"]
    last_err = None
    for filename in candidates:
        try:
            _ = hf_hub_download(
                repo_id=model_id,
                filename=filename,
                trust_env=True,     # é€™è¡Œæ˜¯ä½ æŒ‡å®šçš„é‡é»
                resume_download=True,
            )
            return
        except Exception as e:
            last_err = e
    raise RuntimeError(f"hf_hub_download é€£ç·šæ¸¬è©¦å¤±æ•—ï¼š{last_err}")


def resolve_model_local_path(
    primary: str,
    backups: List[str],
    prefer_local_env: bool = True
) -> Tuple[str, bool]:
    """
    å–å¾—å¯ç”¨çš„æœ¬åœ°æ¨¡å‹è³‡æ–™å¤¾ï¼Œä¸¦åˆ¤æ–·æ˜¯å¦ç‚º e5 ç³»åˆ—ï¼ˆæ±ºå®šæ˜¯å¦åŠ  query/passage å‰ç¶´ï¼‰ã€‚

    å›å‚³ï¼š(local_path, is_e5)
    """
    # 1) è‹¥æœ‰è¨­å®š EMBED_MODEL_DIRï¼Œå„ªå…ˆä½¿ç”¨ï¼ˆå®Œå…¨é›¢ç·šï¼‰
    local_env = os.environ.get("EMBED_MODEL_DIR")
    if prefer_local_env and local_env and os.path.isdir(local_env):
        is_e5 = ("e5" in os.path.basename(local_env).lower()) or ("intfloat" in local_env.lower())
        print(f"ä½¿ç”¨æœ¬åœ° EMBED_MODEL_DIRï¼š{local_env}")
        return local_env, is_e5

    # 2) ä¾åºå˜—è©¦ primary -> backupsï¼Œä¸‹è¼‰åˆ° models/<name>
    candidates = [primary] + list(backups)
    for mid in candidates:
        pretty_name = mid.replace("/", "-")
        local_dir = os.path.join("models", pretty_name)
        os.makedirs("models", exist_ok=True)

        # å·²æœ‰å¿«å–å°±ç›´æ¥ç”¨
        if os.path.isdir(local_dir) and os.listdir(local_dir):
            is_e5 = ("e5" in mid.lower()) or ("intfloat" in mid.lower())
            print(f"åµæ¸¬åˆ°å·²å­˜åœ¨çš„æœ¬åœ°æ¨¡å‹ï¼š{local_dir}")
            return local_dir, is_e5

        # æ²’æœ‰å°±è©¦è‘—ä¸‹è¼‰
        try:
            print(f"â†’ å˜—è©¦ snapshot_downloadï¼š{mid}")
            path = try_snapshot_download(mid, local_dir)
            is_e5 = ("e5" in mid.lower()) or ("intfloat" in mid.lower())
            print(f"âœ… ä¸‹è¼‰å®Œæˆï¼š{mid} -> {path}")
            return path, is_e5
        except Exception as e1:
            print(f"âš ï¸ snapshot_download å¤±æ•—ï¼Œæ”¹ç”¨ hf_hub_download æ¢æ¸¬ proxyï¼š{e1}")
            try:
                # é€™ä¸€æ­¥åªç‚ºäº†èµ°ä¸€æ¬¡ä»£ç†ï¼ŒæŠŠé€šé“æ‰“é€šï¼›ä¸æœƒæ‹¿åˆ°æ•´åŒ…æ¨¡å‹
                gentle_probe_with_hf_hub_download(mid)
                # æ¢æ¸¬æˆåŠŸå¾Œï¼Œå†å›é ­ç”¨ snapshot_download æŠ“æ•´åŒ…
                path = try_snapshot_download(mid, local_dir)
                is_e5 = ("e5" in mid.lower()) or ("intfloat" in mid.lower())
                print(f"âœ… ä¸‹è¼‰å®Œæˆï¼ˆç¶“ hf_hub_download æ¢æ¸¬ï¼‰ï¼š{mid} -> {path}")
                return path, is_e5
            except Exception as e2:
                print(f"âŒ ä¸‹è¼‰ {mid} ä»å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€å€‹å‚™æ´ï¼š{e2}")

    raise RuntimeError(
        "ç„¡æ³•ä¸‹è¼‰ä»»ä½•å€™é¸æ¨¡å‹ã€‚\n"
        "è«‹ç¢ºèªï¼š\n"
        "1) ä»£ç†èˆ‡æ†‘è­‰å·²æ­£ç¢ºè¨­å®šï¼ˆç³»çµ±ç’°å¢ƒè®Šæ•¸ HTTP_PROXY/HTTPS_PROXYï¼‰ã€‚\n"
        "2) æˆ–æ”¹ç”¨é›¢ç·šï¼šå…ˆæŠŠæ¨¡å‹è³‡æ–™å¤¾æ”¾åˆ°æœ¬æ©Ÿï¼Œè¨­å®š EMBED_MODEL_DIR æŒ‡å‘å®ƒã€‚"
    )


# =====================
# æ–‡æœ¬è¼‰å…¥
# =====================
def load_txt_as_documents(path: str) -> List[Document]:
    encodings = ["utf-8", "utf-8-sig", "cp950"]
    last_err = None
    for enc in encodings:
        try:
            with open(path, "r", encoding=enc) as f:
                text = f.read()
            return [Document(page_content=text, metadata={"source": path, "encoding": enc})]
        except UnicodeDecodeError as e:
            last_err = e
    raise UnicodeDecodeError(f"ç„¡æ³•ç”¨ {encodings} ä»»ä¸€ç·¨ç¢¼è®€å–ï¼š{path}\næœ€å¾ŒéŒ¯èª¤ï¼š{last_err}")


def load_all_documents(folder_path: str) -> List[Document]:
    supported_ext = {".txt", ".pdf", ".docx"}
    documents: List[Document] = []
    files = sorted(os.listdir(folder_path))
    if not files:
        print(f"âš ï¸ è³‡æ–™å¤¾ {folder_path} æ˜¯ç©ºçš„ã€‚")
        return documents

    for file in files:
        if file.startswith("."):
            continue
        path = os.path.join(folder_path, file)
        _, ext = os.path.splitext(file.lower())
        if ext not in supported_ext:
            print(f"â†ªï¸ ç•¥éä¸æ”¯æ´æª”æ¡ˆï¼š{file}")
            continue

        try:
            if ext == ".txt":
                docs = load_txt_as_documents(path)
            elif ext == ".pdf":
                loader = PyPDFLoader(path)
                docs = loader.load()
                for d in docs:
                    d.metadata = {**(d.metadata or {}), "source": path}
            elif ext == ".docx":
                loader = Docx2txtLoader(path)
                docs = loader.load()
                for d in docs:
                    d.metadata = {**(d.metadata or {}), "source": path}
            else:
                docs = []
            documents.extend(docs)
            print(f"âœ… è¼‰å…¥æˆåŠŸï¼š{file}ï¼ˆæ–°å¢ {len(docs)} ç­†ï¼‰")
        except Exception as e:
            print(f"âŒ è¼‰å…¥å¤±æ•—ï¼š{file} -> {e}", file=sys.stderr)
    return documents


# =====================
# åµŒå…¥å™¨ï¼ˆç”¨æœ¬åœ°æ¨¡å‹è·¯å¾‘ï¼‰
# =====================
class SentenceTransformerEmbeddings:
    def __init__(self, local_model_dir: str, is_e5: bool):
        self.model = SentenceTransformer(local_model_dir)
        self.is_e5 = is_e5

    def embed_documents(self, texts: List[str]):
        if self.is_e5:
            texts = [f"passage: {t}" for t in texts]
        return self.model.encode(
            texts, show_progress_bar=True, convert_to_tensor=False, normalize_embeddings=True
        )

    def embed_query(self, text: str):
        q = f"query: {text}" if self.is_e5 else text
        return self.model.encode(q, convert_to_tensor=False, normalize_embeddings=True)


# =====================
# ä¸»æµç¨‹
# =====================
def main():
    # 0) é¡¯ç¤ºé—œéµç‰ˆæœ¬ï¼ˆdebugç”¨ï¼‰
    try:
        import langchain, langchain_community
        print("[Versions]",
              "langchain=", getattr(langchain, "__version__", "unknown"),
              "langchain-community=", getattr(langchain_community, "__version__", "unknown"))
    except Exception:
        pass

    # 1) è¼‰å…¥æ–‡ä»¶
    docs = load_all_documents(UPLOAD_DIR)
    if not docs:
        print("âš ï¸ æ²’æœ‰å¯ç”¨æ–‡ä»¶ï¼Œæµç¨‹çµæŸã€‚")
        return

    # 2) åˆ†å‰²
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = splitter.split_documents(docs)
    print(f"ğŸ“„ å·²åˆ‡å‡º {len(split_docs)} å€‹ chunksã€‚")

    # 3) å–å¾—æ¨¡å‹ï¼ˆèµ° proxy / é›¢ç·šçš†å¯ï¼‰
    try:
        local_model_dir, is_e5 = resolve_model_local_path(PRIMARY_MODEL_ID, BACKUP_MODEL_IDS)
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è¼‰/è§£æå¤±æ•—ï¼š{e}", file=sys.stderr)
        return

    # 4) å»ºç«‹å‘é‡åº«
    print(f"ğŸ” ä½¿ç”¨æ¨¡å‹ï¼š{local_model_dir}ï¼ˆis_e5={is_e5}ï¼‰")
    embedding = SentenceTransformerEmbeddings(local_model_dir, is_e5)
    vectorstore = FAISS.from_documents(split_docs, embedding)
    print("âœ… å‘é‡ç´¢å¼•å®Œæˆã€‚")

    # 5) å„²å­˜
    if os.path.exists(DB_DIR):
        shutil.rmtree(DB_DIR)
    vectorstore.save_local(DB_DIR)
    print(f"ğŸ’¾ å·²å„²å­˜å‘é‡åº«ï¼š{DB_DIR}")

    # 6) å£“ç¸®å‚™ä»½
    if os.path.exists(DB_ZIP):
        os.remove(DB_ZIP)
    shutil.make_archive(DB_DIR, 'zip', DB_DIR)
    print(f"ğŸ“¦ å·²è¼¸å‡ºï¼š{DB_ZIP}")


if __name__ == "__main__":
    main()
